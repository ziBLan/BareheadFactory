{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setup"
      ],
      "metadata": {
        "id": "SEvU2up7WH4a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "6vaK0s48v8_w"
      },
      "outputs": [],
      "source": [
        "#@title Setup\n",
        "!git clone https://github.com/oneThousand1000/HairMapper\n",
        "import os\n",
        "os.chdir('./HairMapper')\n",
        "\n",
        "# ==================== download checkpoint ====================\n",
        "!pip install gdown\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "print('Please wait util all models are downloaded...')\n",
        "\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "def download_from_google_drive(file_id,file_dst):\n",
        "  downloaded = drive.CreateFile({'id':file_id})\n",
        "  downloaded.FetchMetadata(fetch_all=True)\n",
        "  downloaded.GetContentFile(file_dst)\n",
        "\n",
        "checkpoints ={\n",
        "    'StyleGAN2-ada-Generator.pth':\n",
        "    {\n",
        "        'url':'1EsGehuEdY4z4t21o2LgW2dSsyN3rxYLJ',\n",
        "        'dir': './ckpts'\n",
        "    },\n",
        "    'e4e_ffhq_encode.pt':\n",
        "    {\n",
        "        'url':'1cUv_reLE6k3604or78EranS7XzuVMWeO',\n",
        "        'dir': './ckpts'\n",
        "    },\n",
        "    'model_ir_se50.pth':\n",
        "    {\n",
        "        'url':'1GIMopzrt2GE_4PG-_YxmVqTQEiaqu5L6',\n",
        "        'dir': './ckpts'\n",
        "    },\n",
        "    'face_parsing.pth':\n",
        "    {\n",
        "        'url':'1IMsrkXA9NuCEy1ij8c8o6wCrAxkmjNPZ',\n",
        "        'dir': './ckpts'\n",
        "    },\n",
        "    'vgg16.pth':\n",
        "    {\n",
        "        'url':'1EPhkEP_1O7ZVk66aBeKoFqf3xiM4BHH8',\n",
        "        'dir': './ckpts'\n",
        "    }\n",
        "}\n",
        "for ckpt in checkpoints:\n",
        "  name = ckpt\n",
        "  url = checkpoints[name]['url']\n",
        "  output_dir = checkpoints[name]['dir']\n",
        "  os.makedirs(output_dir,exist_ok=True)\n",
        "  output_path = os.path.join(output_dir,name)\n",
        "  #gdown.download(url=url,output=output_path,quiet=False) # bug\n",
        "  download_from_google_drive(file_id=url, file_dst=output_path)\n",
        "\n",
        "classification_ckpt =[\n",
        "        {'url':'1SSw6vd-25OGnLAE0kuA-_VHabxlsdLXL',\n",
        "        'dir': './classifier/gender_classification'},\n",
        "        {'url':'1n14ckDcgiy7eu-e9XZhqQYb5025PjSpV',\n",
        "        'dir': './classifier/hair_classification'}\n",
        "]\n",
        "for clf_ckpt_dict in classification_ckpt:\n",
        "  name = 'classification_model.pth'\n",
        "  url = clf_ckpt_dict['url']\n",
        "  output_dir = clf_ckpt_dict['dir']\n",
        "  os.makedirs(output_dir,exist_ok=True)\n",
        "  output_path = os.path.join(output_dir,name)\n",
        "  #gdown.download(url=url,output=output_path,quiet=False)\n",
        "  download_from_google_drive(file_id=url, file_dst=output_path)\n",
        "\n",
        "\n",
        "# ==================== install packages ====================\n",
        "!pip install torch===1.7.1+cu110 torchvision===0.8.2+cu110 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install -r requirements.txt\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: pre-trained models access\n",
        "\n",
        "Please fill out this form for **pre-trained models access**: https://forms.gle/a5pRbE3yxEr7sZDm7\n",
        "\n",
        "Then fill out the mapper_url with the url of [Final HairMapper]\n",
        "\n"
      ],
      "metadata": {
        "id": "Vf0wbY4dVOqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapper_url = '' #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "h0glHNtLWQTN",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: download pre-trained model and test image"
      ],
      "metadata": {
        "id": "By8BOsUlWlVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "code",
        "id": "OSnXgjN8z69C"
      },
      "outputs": [],
      "source": [
        "#@title Download Pretrain Models\n",
        "\n",
        "name = 'best_model.pt'\n",
        "mapper_url = mapper_url.replace('https://drive.google.com/file/d/','')\n",
        "output_dir = './mapper/checkpoints/final'\n",
        "os.makedirs(output_dir,exist_ok=True)\n",
        "output_path = os.path.join(output_dir,name)\n",
        "download_from_google_drive(file_id=mapper_url, file_dst=output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1kPIBkM64Vkq"
      },
      "outputs": [],
      "source": [
        "#@title Download test image\n",
        "name = '00006.png'\n",
        "url = '1wa21vca8P8cq6qVwMmo4Kl4ZySceM2ul'\n",
        "output_dir = './test_data/origin'\n",
        "os.makedirs(output_dir,exist_ok=True)\n",
        "output_path = os.path.join(output_dir,name)\n",
        "download_from_google_drive(file_id=url, file_dst=output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Run encoder"
      ],
      "metadata": {
        "id": "C-kaQzhWWvPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('./encoder4editing')"
      ],
      "metadata": {
        "id": "iiZkiA0jUSGH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please put the real images to **./test_data/origin** (examplar data can be found in ./test_data/origin/00006.png).\n",
        "\n"
      ],
      "metadata": {
        "id": "OYXDMXr6VomP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "code",
        "id": "g48Gc41R49C-"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "import sys\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from PIL import ImageFile\n",
        "import glob\n",
        "import os\n",
        "import argparse\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "from models.psp import pSp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_on_batch(inputs, net):\n",
        "    latents = net(inputs.to(\"cuda\").float(), randomize_noise=False, return_latents=True)\n",
        "    return latents\n"
      ],
      "metadata": {
        "id": "oM97v_b7XG4G"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_transforms = transforms.Compose([\n",
        "          transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]\n",
        "            )\n",
        "model_path = \"../ckpts/e4e_ffhq_encode.pt\"\n",
        "ckpt = torch.load(model_path, map_location='cpu')\n",
        "opts = ckpt['opts']\n",
        "opts['checkpoint_path'] = model_path\n",
        "opts= Namespace(**opts)\n",
        "net = pSp(opts)\n",
        "net.eval()\n",
        "net.cuda()"
      ],
      "metadata": {
        "id": "cRgwRCs9XLan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '../test_data'\n",
        "file_dir = os.path.join(data_dir,'origin')\n",
        "code_dir = os.path.join(data_dir,'code')\n",
        "if not os.path.exists(code_dir):\n",
        "    os.mkdir(code_dir)\n",
        "for file_path in glob.glob(os.path.join(file_dir,'*.png'))+glob.glob(os.path.join(file_dir,'*.jpg')):\n",
        "    name = os.path.basename(file_path)[:-4]\n",
        "    code_path =os.path.join(code_dir,f'{name}.npy')\n",
        "    if os.path.exists(code_path):\n",
        "        continue\n",
        "    input_image = PIL.Image.open(file_path)\n",
        "    transformed_image = img_transforms(input_image)\n",
        "    with torch.no_grad():\n",
        "        latents = run_on_batch(transformed_image.unsqueeze(0), net)\n",
        "        latent = latents[0].cpu().numpy()\n",
        "        latent = np.reshape(latent,(1,18,512))\n",
        "        np.save(code_path,latent)\n",
        "        print(f'save to {code_path}')\n"
      ],
      "metadata": {
        "id": "kEKEdnVUXc7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Run HairMaper"
      ],
      "metadata": {
        "id": "e1RcV6nhWz9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('../')"
      ],
      "metadata": {
        "id": "t_N3sGzZUT-v"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import argparse\n",
        "import cv2\n",
        "from styleGAN2_ada_model.stylegan2_ada_generator import StyleGAN2adaGenerator\n",
        "from tqdm import tqdm\n",
        "from classifier.src.feature_extractor.hair_mask_extractor import get_hair_mask, get_parsingNet\n",
        "from mapper.networks.level_mapper import LevelMapper\n",
        "import torch\n",
        "import glob\n",
        "from diffuse.inverter_remove_hair import InverterRemoveHair\n",
        "import numpy as np\n",
        "from PIL import ImageFile\n",
        "import os\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "metadata": {
        "id": "XPrzEeq8WmfL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'stylegan2_ada'\n",
        "latent_space_type = 'wp'\n",
        "data_dir = './test_data'\n",
        "print(f'Initializing generator.')\n",
        "model = StyleGAN2adaGenerator(model_name, logger=None, truncation_psi=1.0)\n",
        "\n",
        "mapper = LevelMapper(input_dim=512).eval().cuda()\n",
        "ckpt = torch.load('./mapper/checkpoints/final/best_model.pt')\n",
        "alpha = float(ckpt['alpha']) * 1.2\n",
        "mapper.load_state_dict(ckpt['state_dict'], strict=True)\n",
        "kwargs = {'latent_space_type': latent_space_type}\n",
        "parsingNet = get_parsingNet(save_pth='./ckpts/face_parsing.pth')\n",
        "inverter = InverterRemoveHair(\n",
        "        model_name,\n",
        "        Generator=model,\n",
        "        learning_rate=0.01,\n",
        "        reconstruction_loss_weight=1.0,\n",
        "        perceptual_loss_weight=5e-5,\n",
        "        truncation_psi=1.0,\n",
        "        logger=None\n",
        ")\n",
        "\n",
        "code_dir = os.path.join(data_dir, 'code')\n",
        "origin_img_dir = os.path.join(data_dir, 'origin')\n",
        "res_dir = os.path.join(data_dir, 'mapper_res')\n",
        "\n",
        "os.makedirs(res_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "fKm4XYmOXygc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code_list = glob.glob(os.path.join(code_dir, '*.npy'))\n",
        "\n",
        "total_num = len(code_list)\n",
        "\n",
        "print(f'Editing {total_num} samples.')\n",
        "pbar = tqdm(total=total_num)\n",
        "for index in range(total_num):\n",
        "    pbar.update(1)\n",
        "    code_path = code_list[index]\n",
        "    name = os.path.basename(code_path)[:-4]\n",
        "    f_path_png = os.path.join(origin_img_dir, f'{name}.png')\n",
        "    f_path_jpg = os.path.join(origin_img_dir, f'{name}.jpg')\n",
        "    if os.path.exists(os.path.join(res_dir, f'{name}.png')):\n",
        "        continue\n",
        "    if os.path.exists(f_path_png):\n",
        "        origin_img_path = f_path_png\n",
        "    elif os.path.exists(f_path_jpg):\n",
        "        origin_img_path = f_path_jpg\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    latent_codes_origin = np.reshape(np.load(code_path), (1, 18, 512))\n",
        "\n",
        "    mapper_input = latent_codes_origin.copy()\n",
        "    mapper_input_tensor = torch.from_numpy(mapper_input).cuda().float()\n",
        "    edited_latent_codes = latent_codes_origin\n",
        "    edited_latent_codes[:, :8, :] += alpha * mapper(mapper_input_tensor).to('cpu').detach().numpy()\n",
        "\n",
        "    origin_img = cv2.imread(origin_img_path)\n",
        "\n",
        "    outputs = model.easy_style_mixing(latent_codes=edited_latent_codes,\n",
        "                                      style_range=range(7, 18),\n",
        "                                      style_codes=latent_codes_origin,\n",
        "                                      mix_ratio=0.8,\n",
        "                                      **kwargs\n",
        "                                      )\n",
        "\n",
        "    edited_img = outputs['image'][0][:, :, ::-1]\n",
        "\n",
        "    # --remain_ear: preserve the ears in the original input image.\n",
        "    hair_mask = get_hair_mask(img_path=origin_img, net=parsingNet, include_hat=True, include_ear=True)\n",
        "\n",
        "    mask_dilate = cv2.dilate(hair_mask,\n",
        "                             kernel=np.ones((50, 50), np.uint8))\n",
        "    mask_dilate_blur = cv2.blur(mask_dilate, ksize=(30, 30))\n",
        "    mask_dilate_blur = (hair_mask + (255 - hair_mask) / 255 * mask_dilate_blur).astype(np.uint8)\n",
        "\n",
        "    face_mask = 255 - mask_dilate_blur\n",
        "\n",
        "    index = np.where(face_mask > 0)\n",
        "    cy = (np.min(index[0]) + np.max(index[0])) // 2\n",
        "    cx = (np.min(index[1]) + np.max(index[1])) // 2\n",
        "    center = (cx, cy)\n",
        "\n",
        "    res_save_path = os.path.join(res_dir, f'{name}.png')\n",
        "\n",
        "    mixed_clone = cv2.seamlessClone(origin_img, edited_img, face_mask[:, :, 0], center, cv2.NORMAL_CLONE)\n",
        "\n",
        "        \n",
        "    cv2.imwrite(res_save_path, mixed_clone)\n"
      ],
      "metadata": {
        "id": "aUo1IJc-YFcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: visualize results"
      ],
      "metadata": {
        "id": "x60yTlVyYHnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "8GZPqpsMYMmz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for res_path in glob.glob('./test_data/mapper_res/*'):\n",
        "    res_img = cv2.imread(res_path)[:,:,::-1]\n",
        "    input_path = res_path.replace('mapper_res','origin')\n",
        "    input_img = cv2.imread(input_path)[:,:,::-1]\n",
        "    visualize = np.concatenate([res_img,input_img], axis=1)\n",
        "    res_im = Image.fromarray(visualize)\n",
        "    display(res_im)"
      ],
      "metadata": {
        "id": "fNKnXK-QUX2w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HairMapper.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}